{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Else"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mcot\u001b[39;00m \u001b[39mimport\u001b[39;00m Collection\n\u001b[0;32m----> 2\u001b[0m med_qa \u001b[39m=\u001b[39m Collection\u001b[39m.\u001b[39;49mload_thoughtsource_100(names\u001b[39m=\u001b[39;49m[\u001b[39m'\u001b[39;49m\u001b[39mmed_qa\u001b[39;49m\u001b[39m'\u001b[39;49m],load_pregenerated_cots\u001b[39m=\u001b[39;49m\u001b[39mFalse\u001b[39;49;00m)\n",
      "File \u001b[0;32m~/ThoughtSource/libs/cot/cot/dataloader.py:447\u001b[0m, in \u001b[0;36mCollection.load_thoughtsource_100\u001b[0;34m(names, load_pregenerated_cots)\u001b[0m\n\u001b[1;32m    445\u001b[0m path_to_biodatasets \u001b[39m=\u001b[39m (pathlib\u001b[39m.\u001b[39mPath(\u001b[39m__file__\u001b[39m)\u001b[39m.\u001b[39mparent\u001b[39m.\u001b[39mabsolute() \u001b[39m/\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mdatasets\u001b[39m\u001b[39m\"\u001b[39m)\u001b[39m.\u001b[39mresolve()\n\u001b[1;32m    446\u001b[0m path_to_thoughtsource_100 \u001b[39m=\u001b[39m path_to_biodatasets \u001b[39m/\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mthoughtsource\u001b[39m\u001b[39m\"\u001b[39m \u001b[39m/\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mthoughtsource_100.json\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m--> 447\u001b[0m collection \u001b[39m=\u001b[39m Collection\u001b[39m.\u001b[39;49mfrom_json(\u001b[39mstr\u001b[39;49m(path_to_thoughtsource_100))\n\u001b[1;32m    448\u001b[0m \u001b[39m# drop all names that are not in the list\u001b[39;00m\n\u001b[1;32m    449\u001b[0m \u001b[39mif\u001b[39;00m names \u001b[39m!=\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mall\u001b[39m\u001b[39m\"\u001b[39m:\n",
      "File \u001b[0;32m~/ThoughtSource/libs/cot/cot/dataloader.py:411\u001b[0m, in \u001b[0;36mCollection.from_json\u001b[0;34m(path_or_json, download_mode, source)\u001b[0m\n\u001b[1;32m    409\u001b[0m collection \u001b[39m=\u001b[39m Collection()\n\u001b[1;32m    410\u001b[0m \u001b[39mfor\u001b[39;00m dataset_name \u001b[39min\u001b[39;00m content\u001b[39m.\u001b[39mkeys():\n\u001b[0;32m--> 411\u001b[0m     info \u001b[39m=\u001b[39m ds\u001b[39m.\u001b[39;49mload_dataset_builder(\n\u001b[1;32m    412\u001b[0m         \u001b[39mstr\u001b[39;49m(scripts[dataset_name]), name\u001b[39m=\u001b[39;49m\u001b[39m\"\u001b[39;49m\u001b[39msource\u001b[39;49m\u001b[39m\"\u001b[39;49m \u001b[39mif\u001b[39;49;00m source \u001b[39melse\u001b[39;49;00m \u001b[39m\"\u001b[39;49m\u001b[39mthoughtsource\u001b[39;49m\u001b[39m\"\u001b[39;49m, download_mode\u001b[39m=\u001b[39;49mdownload_mode\n\u001b[1;32m    413\u001b[0m     )\u001b[39m.\u001b[39minfo\n\u001b[1;32m    414\u001b[0m     dataset_dict \u001b[39m=\u001b[39m \u001b[39mdict\u001b[39m()\n\u001b[1;32m    415\u001b[0m     \u001b[39mfor\u001b[39;00m split_name \u001b[39min\u001b[39;00m content[dataset_name]\u001b[39m.\u001b[39mkeys():\n",
      "File \u001b[0;32m~/ThoughtSource/.conda/lib/python3.9/site-packages/datasets/load.py:1490\u001b[0m, in \u001b[0;36mload_dataset_builder\u001b[0;34m(path, name, data_dir, data_files, cache_dir, features, download_config, download_mode, revision, use_auth_token, **config_kwargs)\u001b[0m\n\u001b[1;32m   1488\u001b[0m     download_config \u001b[39m=\u001b[39m download_config\u001b[39m.\u001b[39mcopy() \u001b[39mif\u001b[39;00m download_config \u001b[39melse\u001b[39;00m DownloadConfig()\n\u001b[1;32m   1489\u001b[0m     download_config\u001b[39m.\u001b[39muse_auth_token \u001b[39m=\u001b[39m use_auth_token\n\u001b[0;32m-> 1490\u001b[0m dataset_module \u001b[39m=\u001b[39m dataset_module_factory(\n\u001b[1;32m   1491\u001b[0m     path,\n\u001b[1;32m   1492\u001b[0m     revision\u001b[39m=\u001b[39;49mrevision,\n\u001b[1;32m   1493\u001b[0m     download_config\u001b[39m=\u001b[39;49mdownload_config,\n\u001b[1;32m   1494\u001b[0m     download_mode\u001b[39m=\u001b[39;49mdownload_mode,\n\u001b[1;32m   1495\u001b[0m     data_dir\u001b[39m=\u001b[39;49mdata_dir,\n\u001b[1;32m   1496\u001b[0m     data_files\u001b[39m=\u001b[39;49mdata_files,\n\u001b[1;32m   1497\u001b[0m )\n\u001b[1;32m   1499\u001b[0m \u001b[39m# Get dataset builder class from the processing script\u001b[39;00m\n\u001b[1;32m   1500\u001b[0m builder_cls \u001b[39m=\u001b[39m import_main_class(dataset_module\u001b[39m.\u001b[39mmodule_path)\n",
      "File \u001b[0;32m~/ThoughtSource/.conda/lib/python3.9/site-packages/datasets/load.py:1160\u001b[0m, in \u001b[0;36mdataset_module_factory\u001b[0;34m(path, revision, download_config, download_mode, force_local_path, dynamic_modules_path, data_dir, data_files, **download_kwargs)\u001b[0m\n\u001b[1;32m   1158\u001b[0m \u001b[39melif\u001b[39;00m path\u001b[39m.\u001b[39mendswith(filename):\n\u001b[1;32m   1159\u001b[0m     \u001b[39mif\u001b[39;00m os\u001b[39m.\u001b[39mpath\u001b[39m.\u001b[39misfile(path):\n\u001b[0;32m-> 1160\u001b[0m         \u001b[39mreturn\u001b[39;00m LocalDatasetModuleFactoryWithScript(\n\u001b[1;32m   1161\u001b[0m             path, download_mode\u001b[39m=\u001b[39;49mdownload_mode, dynamic_modules_path\u001b[39m=\u001b[39;49mdynamic_modules_path\n\u001b[1;32m   1162\u001b[0m         )\u001b[39m.\u001b[39;49mget_module()\n\u001b[1;32m   1163\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[1;32m   1164\u001b[0m         \u001b[39mraise\u001b[39;00m \u001b[39mFileNotFoundError\u001b[39;00m(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mCouldn\u001b[39m\u001b[39m'\u001b[39m\u001b[39mt find a dataset script at \u001b[39m\u001b[39m{\u001b[39;00mrelative_to_absolute_path(path)\u001b[39m}\u001b[39;00m\u001b[39m\"\u001b[39m)\n",
      "File \u001b[0;32m~/ThoughtSource/.conda/lib/python3.9/site-packages/datasets/load.py:700\u001b[0m, in \u001b[0;36mLocalDatasetModuleFactoryWithScript.get_module\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    698\u001b[0m dataset_infos_path \u001b[39m=\u001b[39m Path(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mpath)\u001b[39m.\u001b[39mparent \u001b[39m/\u001b[39m config\u001b[39m.\u001b[39mDATASETDICT_INFOS_FILENAME\n\u001b[1;32m    699\u001b[0m imports \u001b[39m=\u001b[39m get_imports(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mpath)\n\u001b[0;32m--> 700\u001b[0m local_imports \u001b[39m=\u001b[39m _download_additional_modules(\n\u001b[1;32m    701\u001b[0m     name\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mname,\n\u001b[1;32m    702\u001b[0m     base_path\u001b[39m=\u001b[39;49m\u001b[39mstr\u001b[39;49m(Path(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mpath)\u001b[39m.\u001b[39;49mparent),\n\u001b[1;32m    703\u001b[0m     imports\u001b[39m=\u001b[39;49mimports,\n\u001b[1;32m    704\u001b[0m     download_config\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mdownload_config,\n\u001b[1;32m    705\u001b[0m )\n\u001b[1;32m    706\u001b[0m additional_files \u001b[39m=\u001b[39m (\n\u001b[1;32m    707\u001b[0m     [(config\u001b[39m.\u001b[39mDATASETDICT_INFOS_FILENAME, \u001b[39mstr\u001b[39m(dataset_infos_path))] \u001b[39mif\u001b[39;00m dataset_infos_path\u001b[39m.\u001b[39mis_file() \u001b[39melse\u001b[39;00m []\n\u001b[1;32m    708\u001b[0m )\n\u001b[1;32m    709\u001b[0m \u001b[39m# copy the script and the files in an importable directory\u001b[39;00m\n",
      "File \u001b[0;32m~/ThoughtSource/.conda/lib/python3.9/site-packages/datasets/load.py:294\u001b[0m, in \u001b[0;36m_download_additional_modules\u001b[0;34m(name, base_path, imports, download_config)\u001b[0m\n\u001b[1;32m    292\u001b[0m \u001b[39mfor\u001b[39;00m library_import_name, library_import_path \u001b[39min\u001b[39;00m library_imports:\n\u001b[1;32m    293\u001b[0m     \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m--> 294\u001b[0m         lib \u001b[39m=\u001b[39m importlib\u001b[39m.\u001b[39;49mimport_module(library_import_name)  \u001b[39m# noqa F841\u001b[39;00m\n\u001b[1;32m    295\u001b[0m     \u001b[39mexcept\u001b[39;00m \u001b[39mImportError\u001b[39;00m:\n\u001b[1;32m    296\u001b[0m         needs_to_be_installed\u001b[39m.\u001b[39mappend((library_import_name, library_import_path))\n",
      "File \u001b[0;32m~/ThoughtSource/.conda/lib/python3.9/importlib/__init__.py:127\u001b[0m, in \u001b[0;36mimport_module\u001b[0;34m(name, package)\u001b[0m\n\u001b[1;32m    125\u001b[0m             \u001b[39mbreak\u001b[39;00m\n\u001b[1;32m    126\u001b[0m         level \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m \u001b[39m1\u001b[39m\n\u001b[0;32m--> 127\u001b[0m \u001b[39mreturn\u001b[39;00m _bootstrap\u001b[39m.\u001b[39;49m_gcd_import(name[level:], package, level)\n",
      "File \u001b[0;32m<frozen importlib._bootstrap>:1030\u001b[0m, in \u001b[0;36m_gcd_import\u001b[0;34m(name, package, level)\u001b[0m\n",
      "File \u001b[0;32m<frozen importlib._bootstrap>:1007\u001b[0m, in \u001b[0;36m_find_and_load\u001b[0;34m(name, import_)\u001b[0m\n",
      "File \u001b[0;32m<frozen importlib._bootstrap>:986\u001b[0m, in \u001b[0;36m_find_and_load_unlocked\u001b[0;34m(name, import_)\u001b[0m\n",
      "File \u001b[0;32m<frozen importlib._bootstrap>:680\u001b[0m, in \u001b[0;36m_load_unlocked\u001b[0;34m(spec)\u001b[0m\n",
      "File \u001b[0;32m<frozen importlib._bootstrap_external>:850\u001b[0m, in \u001b[0;36mexec_module\u001b[0;34m(self, module)\u001b[0m\n",
      "File \u001b[0;32m<frozen importlib._bootstrap>:228\u001b[0m, in \u001b[0;36m_call_with_frames_removed\u001b[0;34m(f, *args, **kwds)\u001b[0m\n",
      "File \u001b[0;32m~/ThoughtSource/.conda/lib/python3.9/site-packages/nltk/__init__.py:134\u001b[0m\n\u001b[1;32m    132\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mnltk\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mcollocations\u001b[39;00m \u001b[39mimport\u001b[39;00m \u001b[39m*\u001b[39m\n\u001b[1;32m    133\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mnltk\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mdecorators\u001b[39;00m \u001b[39mimport\u001b[39;00m decorator, memoize\n\u001b[0;32m--> 134\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mnltk\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mfeatstruct\u001b[39;00m \u001b[39mimport\u001b[39;00m \u001b[39m*\u001b[39m\n\u001b[1;32m    135\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mnltk\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mgrammar\u001b[39;00m \u001b[39mimport\u001b[39;00m \u001b[39m*\u001b[39m\n\u001b[1;32m    136\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mnltk\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mprobability\u001b[39;00m \u001b[39mimport\u001b[39;00m \u001b[39m*\u001b[39m\n",
      "File \u001b[0;32m~/ThoughtSource/.conda/lib/python3.9/site-packages/nltk/featstruct.py:97\u001b[0m\n\u001b[1;32m     94\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mfunctools\u001b[39;00m \u001b[39mimport\u001b[39;00m total_ordering\n\u001b[1;32m     96\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mnltk\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39minternals\u001b[39;00m \u001b[39mimport\u001b[39;00m raise_unorderable_types, read_str\n\u001b[0;32m---> 97\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mnltk\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39msem\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mlogic\u001b[39;00m \u001b[39mimport\u001b[39;00m (\n\u001b[1;32m     98\u001b[0m     Expression,\n\u001b[1;32m     99\u001b[0m     LogicalExpressionException,\n\u001b[1;32m    100\u001b[0m     LogicParser,\n\u001b[1;32m    101\u001b[0m     SubstituteBindingsI,\n\u001b[1;32m    102\u001b[0m     Variable,\n\u001b[1;32m    103\u001b[0m )\n\u001b[1;32m    105\u001b[0m \u001b[39m######################################################################\u001b[39;00m\n\u001b[1;32m    106\u001b[0m \u001b[39m# Feature Structure\u001b[39;00m\n\u001b[1;32m    107\u001b[0m \u001b[39m######################################################################\u001b[39;00m\n\u001b[1;32m    110\u001b[0m \u001b[39m@total_ordering\u001b[39m\n\u001b[1;32m    111\u001b[0m \u001b[39mclass\u001b[39;00m \u001b[39mFeatStruct\u001b[39;00m(SubstituteBindingsI):\n",
      "File \u001b[0;32m~/ThoughtSource/.conda/lib/python3.9/site-packages/nltk/sem/__init__.py:44\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[39m# Natural Language Toolkit: Semantic Interpretation\u001b[39;00m\n\u001b[1;32m      2\u001b[0m \u001b[39m#\u001b[39;00m\n\u001b[1;32m      3\u001b[0m \u001b[39m# Copyright (C) 2001-2022 NLTK Project\u001b[39;00m\n\u001b[1;32m      4\u001b[0m \u001b[39m# Author: Ewan Klein <ewan@inf.ed.ac.uk>\u001b[39;00m\n\u001b[1;32m      5\u001b[0m \u001b[39m# URL: <https://www.nltk.org/>\u001b[39;00m\n\u001b[1;32m      6\u001b[0m \u001b[39m# For license information, see LICENSE.TXT\u001b[39;00m\n\u001b[1;32m      8\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m      9\u001b[0m \u001b[39mNLTK Semantic Interpretation Package\u001b[39;00m\n\u001b[1;32m     10\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     41\u001b[0m \u001b[39m    >>> m = Model(dom, val)\u001b[39;00m\n\u001b[1;32m     42\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[0;32m---> 44\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mnltk\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39msem\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mboxer\u001b[39;00m \u001b[39mimport\u001b[39;00m Boxer\n\u001b[1;32m     45\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mnltk\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39msem\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mdrt\u001b[39;00m \u001b[39mimport\u001b[39;00m DRS, DrtExpression\n\u001b[1;32m     46\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mnltk\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39msem\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mevaluate\u001b[39;00m \u001b[39mimport\u001b[39;00m (\n\u001b[1;32m     47\u001b[0m     Assignment,\n\u001b[1;32m     48\u001b[0m     Model,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     54\u001b[0m     set2rel,\n\u001b[1;32m     55\u001b[0m )\n",
      "File \u001b[0;32m~/ThoughtSource/.conda/lib/python3.9/site-packages/nltk/sem/boxer.py:40\u001b[0m\n\u001b[1;32m     37\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39moptparse\u001b[39;00m \u001b[39mimport\u001b[39;00m OptionParser\n\u001b[1;32m     39\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mnltk\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39minternals\u001b[39;00m \u001b[39mimport\u001b[39;00m find_binary\n\u001b[0;32m---> 40\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mnltk\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39msem\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mdrt\u001b[39;00m \u001b[39mimport\u001b[39;00m (\n\u001b[1;32m     41\u001b[0m     DRS,\n\u001b[1;32m     42\u001b[0m     DrtApplicationExpression,\n\u001b[1;32m     43\u001b[0m     DrtEqualityExpression,\n\u001b[1;32m     44\u001b[0m     DrtNegatedExpression,\n\u001b[1;32m     45\u001b[0m     DrtOrExpression,\n\u001b[1;32m     46\u001b[0m     DrtParser,\n\u001b[1;32m     47\u001b[0m     DrtProposition,\n\u001b[1;32m     48\u001b[0m     DrtTokens,\n\u001b[1;32m     49\u001b[0m     DrtVariableExpression,\n\u001b[1;32m     50\u001b[0m )\n\u001b[1;32m     51\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mnltk\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39msem\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mlogic\u001b[39;00m \u001b[39mimport\u001b[39;00m (\n\u001b[1;32m     52\u001b[0m     ExpectedMoreTokensException,\n\u001b[1;32m     53\u001b[0m     LogicalExpressionException,\n\u001b[1;32m     54\u001b[0m     UnexpectedTokenException,\n\u001b[1;32m     55\u001b[0m     Variable,\n\u001b[1;32m     56\u001b[0m )\n\u001b[1;32m     59\u001b[0m \u001b[39mclass\u001b[39;00m \u001b[39mBoxer\u001b[39;00m:\n",
      "File \u001b[0;32m~/ThoughtSource/.conda/lib/python3.9/site-packages/nltk/sem/drt.py:43\u001b[0m\n\u001b[1;32m     41\u001b[0m \u001b[39m# Import Tkinter-based modules if they are available\u001b[39;00m\n\u001b[1;32m     42\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m---> 43\u001b[0m     \u001b[39mfrom\u001b[39;00m \u001b[39mtkinter\u001b[39;00m \u001b[39mimport\u001b[39;00m Canvas, Tk\n\u001b[1;32m     44\u001b[0m     \u001b[39mfrom\u001b[39;00m \u001b[39mtkinter\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mfont\u001b[39;00m \u001b[39mimport\u001b[39;00m Font\n\u001b[1;32m     46\u001b[0m     \u001b[39mfrom\u001b[39;00m \u001b[39mnltk\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mutil\u001b[39;00m \u001b[39mimport\u001b[39;00m in_idle\n",
      "File \u001b[0;32m~/ThoughtSource/.conda/lib/python3.9/tkinter/__init__.py:37\u001b[0m\n\u001b[1;32m     34\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39msys\u001b[39;00m\n\u001b[1;32m     35\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39mtypes\u001b[39;00m\n\u001b[0;32m---> 37\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39m_tkinter\u001b[39;00m \u001b[39m# If this fails your Python may not be configured for Tk\u001b[39;00m\n\u001b[1;32m     38\u001b[0m TclError \u001b[39m=\u001b[39m _tkinter\u001b[39m.\u001b[39mTclError\n\u001b[1;32m     39\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mtkinter\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mconstants\u001b[39;00m \u001b[39mimport\u001b[39;00m \u001b[39m*\u001b[39m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "from cot import Collection\n",
    "med_qa = Collection.load_thoughtsource_100(names=['med_qa'],load_pregenerated_cots=False)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "| Name   | Train   | Valid   |   Test |\n",
       "|--------|---------|---------|--------|\n",
       "| med_qa | -       | -       |    100 |\n",
       "\n",
       "Not loaded: ['aqua', 'asdiv', 'commonsense_qa', 'entailment_bank', 'gsm8k', 'mawps', 'med_qa_open', 'medmc_qa', '_init_', 'mmlu_clinical_knowledge', 'mmlu_college_biology', 'mmlu_college_medicine', 'mmlu_medical_genetics', 'mmlu_professional_medicine', '_init_', 'mmlu_anatomy', 'open_book_qa', 'pubmed_qa', 'qed', 'strategy_qa', 'svamp', 'worldtree']"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "med_qa"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "config={\n",
    "    \"instruction_keys\": None,\n",
    "    \"cot_trigger_keys\": [\"zhou-01\"],\n",
    "    \"answer_extraction_keys\": 'auto-kojima', \n",
    "    \"author\" : \"thoughtsource\",\n",
    "    \"api_service\": \"openai_chat\",\n",
    "    \"api_time_interval\": 1,\n",
    "    \"engine\": \"gpt-3.5-turbo\", \n",
    "    \"temperature\": 0,\n",
    "    \"max_tokens\": 512,\n",
    "    \"verbose\": False,\n",
    "    \"warn\": False,\n",
    "}\n",
    "med_qa.generate(config=config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "de4eca722203420fa2d28f332796ceaf",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Creating json from Arrow format:   0%|          | 0/1 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7fe90e6cf80c4df9b782e4f33a86d797",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/100 [00:00<?, ?ex/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "dde4c3a3a82b443ea998d13aca758c6c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Creating json from Arrow format:   0%|          | 0/1 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "med_qa.dump(\"med_qa_zhou.json\")\n",
    "med_qa.evaluate()\n",
    "med_qa.dump(\"med_qa_zhou_eval.json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "43a5d52754644c4395e24d4213badbc5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/100 [00:00<?, ?ex/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "{'med_qa': {'test': {'accuracy': {'gpt-3.5-turbo': {'None_zhou-01_kojima-A-E': 0.58}}}}}"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "med_qa.evaluate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating med_qa...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3c420a6f464545f8ade61162d48c87bf",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/100 [00:00<?, ?ex/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Retrying langchain.chat_models.openai.ChatOpenAI.completion_with_retry.<locals>._completion_with_retry in 4.0 seconds as it raised RateLimitError: That model is currently overloaded with other requests. You can retry your request, or contact us through our help center at help.openai.com if the error persists. (Please include the request ID cc9ca491264f0eb1cc2357d0f2e4cee9 in your message.).\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5cfd846d808a4ee28a7113da28369830",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Creating json from Arrow format:   0%|          | 0/1 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ebaecb3bd1fa4fcdb9d284f6d43caefc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/100 [00:00<?, ?ex/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7aeaa4500a254d41b86ad1292c8cd00b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Creating json from Arrow format:   0%|          | 0/1 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b715e84218ca430fb6e429fe6944dc05",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/100 [00:00<?, ?ex/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "{'med_qa': {'test': {'accuracy': {'gpt-4': {'None_zhou-01_kojima-A-E': 0.81}}}}}"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "config={\n",
    "    \"instruction_keys\": None,\n",
    "    \"cot_trigger_keys\": [\"zhou-01\"],\n",
    "    \"answer_extraction_keys\": 'auto-kojima', \n",
    "    \"author\" : \"thoughtsource\",\n",
    "    \"api_service\": \"openai_chat\",\n",
    "    \"api_time_interval\": 1,\n",
    "    \"engine\": \"gpt-4\", \n",
    "    \"temperature\": 0,\n",
    "    \"max_tokens\": 512,\n",
    "    \"verbose\": False,\n",
    "    \"warn\": False,\n",
    "}\n",
    "med_qa.generate(config=config)\n",
    "\n",
    "med_qa.dump(\"med_qa_zhou_gpt4.json\")\n",
    "med_qa.evaluate()\n",
    "med_qa.dump(\"med_qa_zhou_eval_gpt4.json\")\n",
    "\n",
    "med_qa.evaluate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "med_qa_correct = med_qa\n",
    "med_qa_false = Collection.from_json(\"/Users/robertpraas/Desktop/ThoughtSource/notebooks/reflection_messages/med_qa_zhou_eval_gpt4.json\")\n",
    "med_qa_correct.select_generated_cots(answer=True)\n",
    "med_qa_correct = med_qa_correct.filter(lambda x: len(x[\"generated_cot\"])==1)\n",
    "\n",
    "med_qa_false.select_generated_cots(answer=False)\n",
    "med_qa_false = med_qa_false.filter(lambda x: len(x[\"generated_cot\"])==1)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "| Name   | Train   | Valid   |   Test |\n",
       "|--------|---------|---------|--------|\n",
       "| med_qa | -       | -       |     19 |\n",
       "\n",
       "Not loaded: ['aqua', 'asdiv', 'commonsense_qa', 'entailment_bank', 'gsm8k', 'mawps', 'med_qa_open', 'medmc_qa', '_init_', 'mmlu_clinical_knowledge', 'mmlu_college_biology', 'mmlu_college_medicine', 'mmlu_medical_genetics', 'mmlu_professional_medicine', '_init_', 'mmlu_anatomy', 'open_book_qa', 'pubmed_qa', 'qed', 'strategy_qa', 'svamp', 'worldtree']"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "med_qa_false"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "| Name   | Train   | Valid   |   Test |\n",
       "|--------|---------|---------|--------|\n",
       "| med_qa | -       | -       |     81 |\n",
       "\n",
       "Not loaded: ['aqua', 'asdiv', 'commonsense_qa', 'entailment_bank', 'gsm8k', 'mawps', 'med_qa_open', 'medmc_qa', '_init_', 'mmlu_clinical_knowledge', 'mmlu_college_biology', 'mmlu_college_medicine', 'mmlu_medical_genetics', 'mmlu_professional_medicine', '_init_', 'mmlu_anatomy', 'open_book_qa', 'pubmed_qa', 'qed', 'strategy_qa', 'svamp', 'worldtree']"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "med_qa_correct"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['apple', 'cherry', 'banana']\n",
      "['banana', 'apple', 'cherry']\n",
      "['cherry', 'apple', 'banana']\n",
      "['apple', 'banana', 'cherry']\n",
      "['apple', 'banana', 'cherry']\n",
      "['banana', 'apple', 'cherry']\n",
      "['banana', 'apple', 'cherry']\n",
      "['banana', 'cherry', 'apple']\n",
      "['cherry', 'apple', 'banana']\n",
      "['cherry', 'banana', 'apple']\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "\n",
    "mylist = [\"apple\", \"banana\", \"cherry\"]\n",
    "\n",
    "for i in range(10):\n",
    "    random.shuffle(mylist)\n",
    "    print(mylist)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "gpt_4_before = Collection.from_json(\"/Users/robertpraas/Desktop/ThoughtSource/notebooks/internal_documentation/paper_2/filtered_ts_100_gpt-4_None_correct_zhou-01_false.json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "| Name           | Train   | Valid   | Test   |\n",
       "|----------------|---------|---------|--------|\n",
       "| commonsense_qa | -       | 7       | -      |\n",
       "| med_qa         | -       | -       | 6      |\n",
       "| medmc_qa       | -       | 8       | -      |\n",
       "| open_book_qa   | -       | -       | 2      |\n",
       "| strategy_qa    | 3       | -       | -      |\n",
       "\n",
       "Not loaded: ['aqua', 'asdiv', 'entailment_bank', 'gsm8k', 'mawps', 'med_qa_open', '_init_', 'mmlu_clinical_knowledge', 'mmlu_college_biology', 'mmlu_college_medicine', 'mmlu_medical_genetics', 'mmlu_professional_medicine', '_init_', 'mmlu_anatomy', 'pubmed_qa', 'qed', 'svamp', 'worldtree']"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gpt_4_before"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gpt_4_before_false = Collection.from_json(\"/Users/robertpraas/Desktop/ThoughtSource/notebooks/internal_documentation/paper_2/filtered_ts_100_gpt-4_None_correct_zhou-01_correct.json\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: '/Users/robertpraas/Desktop/ThoughtSource/libs/cot/cot/datasets/thoughtsource/thoughtsource_100.json'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m/Users/robertpraas/Desktop/ThoughtSource/notebooks/reflection_messages/data_helper_notebook.ipynb Cell 14\u001b[0m in \u001b[0;36m<cell line: 2>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/robertpraas/Desktop/ThoughtSource/notebooks/reflection_messages/data_helper_notebook.ipynb#X16sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mcot\u001b[39;00m \u001b[39mimport\u001b[39;00m Collection\n\u001b[0;32m----> <a href='vscode-notebook-cell:/Users/robertpraas/Desktop/ThoughtSource/notebooks/reflection_messages/data_helper_notebook.ipynb#X16sZmlsZQ%3D%3D?line=1'>2</a>\u001b[0m medmc \u001b[39m=\u001b[39m Collection\u001b[39m.\u001b[39;49mload_thoughtsource_100(names \u001b[39m=\u001b[39;49m [\u001b[39m'\u001b[39;49m\u001b[39mmedmc_qa\u001b[39;49m\u001b[39m'\u001b[39;49m],load_pregenerated_cots\u001b[39m=\u001b[39;49m\u001b[39mFalse\u001b[39;49;00m)\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/robertpraas/Desktop/ThoughtSource/notebooks/reflection_messages/data_helper_notebook.ipynb#X16sZmlsZQ%3D%3D?line=2'>3</a>\u001b[0m openbook \u001b[39m=\u001b[39m Collection\u001b[39m.\u001b[39mload_thoughtsource_100(names \u001b[39m=\u001b[39m [\u001b[39m'\u001b[39m\u001b[39mopen_book_qa\u001b[39m\u001b[39m'\u001b[39m],load_pregenerated_cots\u001b[39m=\u001b[39m\u001b[39mFalse\u001b[39;00m)\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/robertpraas/Desktop/ThoughtSource/notebooks/reflection_messages/data_helper_notebook.ipynb#X16sZmlsZQ%3D%3D?line=3'>4</a>\u001b[0m worldtree \u001b[39m=\u001b[39m Collection\u001b[39m.\u001b[39mload_thoughtsource_100(names \u001b[39m=\u001b[39m [\u001b[39m'\u001b[39m\u001b[39mworldtree\u001b[39m\u001b[39m'\u001b[39m],load_pregenerated_cots\u001b[39m=\u001b[39m\u001b[39mFalse\u001b[39;00m)\n",
      "File \u001b[0;32m~/Desktop/ThoughtSource/libs/cot/cot/dataloader.py:447\u001b[0m, in \u001b[0;36mCollection.load_thoughtsource_100\u001b[0;34m(names, load_pregenerated_cots)\u001b[0m\n\u001b[1;32m    445\u001b[0m path_to_biodatasets \u001b[39m=\u001b[39m (pathlib\u001b[39m.\u001b[39mPath(\u001b[39m__file__\u001b[39m)\u001b[39m.\u001b[39mparent\u001b[39m.\u001b[39mabsolute() \u001b[39m/\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mdatasets\u001b[39m\u001b[39m\"\u001b[39m)\u001b[39m.\u001b[39mresolve()\n\u001b[1;32m    446\u001b[0m path_to_thoughtsource_100 \u001b[39m=\u001b[39m path_to_biodatasets \u001b[39m/\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mthoughtsource\u001b[39m\u001b[39m\"\u001b[39m \u001b[39m/\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mthoughtsource_100.json\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m--> 447\u001b[0m collection \u001b[39m=\u001b[39m Collection\u001b[39m.\u001b[39;49mfrom_json(\u001b[39mstr\u001b[39;49m(path_to_thoughtsource_100))\n\u001b[1;32m    448\u001b[0m \u001b[39m# drop all names that are not in the list\u001b[39;00m\n\u001b[1;32m    449\u001b[0m \u001b[39mif\u001b[39;00m names \u001b[39m!=\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mall\u001b[39m\u001b[39m\"\u001b[39m:\n",
      "File \u001b[0;32m~/Desktop/ThoughtSource/libs/cot/cot/dataloader.py:403\u001b[0m, in \u001b[0;36mCollection.from_json\u001b[0;34m(path_or_json, download_mode, source)\u001b[0m\n\u001b[1;32m    401\u001b[0m                 content \u001b[39m=\u001b[39m json\u001b[39m.\u001b[39mload(infile)\n\u001b[1;32m    402\u001b[0m         \u001b[39melse\u001b[39;00m:\n\u001b[0;32m--> 403\u001b[0m             \u001b[39mraise\u001b[39;00m e\n\u001b[1;32m    404\u001b[0m \u001b[39melif\u001b[39;00m \u001b[39misinstance\u001b[39m(path_or_json, \u001b[39mdict\u001b[39m):\n\u001b[1;32m    405\u001b[0m     content \u001b[39m=\u001b[39m path_or_json\n",
      "File \u001b[0;32m~/Desktop/ThoughtSource/libs/cot/cot/dataloader.py:394\u001b[0m, in \u001b[0;36mCollection.from_json\u001b[0;34m(path_or_json, download_mode, source)\u001b[0m\n\u001b[1;32m    392\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39misinstance\u001b[39m(path_or_json, \u001b[39mstr\u001b[39m):\n\u001b[1;32m    393\u001b[0m     \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m--> 394\u001b[0m         \u001b[39mwith\u001b[39;00m \u001b[39mopen\u001b[39;49m(path_or_json, \u001b[39m\"\u001b[39;49m\u001b[39mr\u001b[39;49m\u001b[39m\"\u001b[39;49m) \u001b[39mas\u001b[39;00m infile:\n\u001b[1;32m    395\u001b[0m             content \u001b[39m=\u001b[39m json\u001b[39m.\u001b[39mload(infile)\n\u001b[1;32m    396\u001b[0m     \u001b[39m# if file is not found and path does not end with .json, try to load it with .json\u001b[39;00m\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '/Users/robertpraas/Desktop/ThoughtSource/libs/cot/cot/datasets/thoughtsource/thoughtsource_100.json'"
     ]
    }
   ],
   "source": [
    "from cot import Collection\n",
    "medmc = Collection.load_thoughtsource_100(names = ['medmc_qa'],load_pregenerated_cots=False)\n",
    "openbook = Collection.load_thoughtsource_100(names = ['open_book_qa'],load_pregenerated_cots=False)\n",
    "worldtree = Collection.load_thoughtsource_100(names = ['worldtree'],load_pregenerated_cots=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating medmc_qa...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "31afe05b5dec42bd99b785f2717957c3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/100 [00:00<?, ?ex/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Retrying langchain.chat_models.openai.ChatOpenAI.completion_with_retry.<locals>._completion_with_retry in 4.0 seconds as it raised RateLimitError: That model is currently overloaded with other requests. You can retry your request, or contact us through our help center at help.openai.com if the error persists. (Please include the request ID 65dec95714c094c243b02b6eb677f7c7 in your message.).\n",
      "Retrying langchain.chat_models.openai.ChatOpenAI.completion_with_retry.<locals>._completion_with_retry in 4.0 seconds as it raised RateLimitError: That model is currently overloaded with other requests. You can retry your request, or contact us through our help center at help.openai.com if the error persists. (Please include the request ID 7d8debf5f8549ccfdb050ea6262c6c76 in your message.).\n",
      "Retrying langchain.chat_models.openai.ChatOpenAI.completion_with_retry.<locals>._completion_with_retry in 4.0 seconds as it raised RateLimitError: That model is currently overloaded with other requests. You can retry your request, or contact us through our help center at help.openai.com if the error persists. (Please include the request ID 76f096a06668ec5a39ea6905a07ecbff in your message.).\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3e09df2992634c22afac42a790762a20",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Creating json from Arrow format:   0%|          | 0/1 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "88e7a12e0c8b48aaac58b8b7c4ed23f5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/100 [00:00<?, ?ex/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0b80fb3f4a2943cdb9b106e608422878",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Creating json from Arrow format:   0%|          | 0/1 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating open_book_qa...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "27861d51a620428cbee4405f80827607",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/100 [00:00<?, ?ex/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Retrying langchain.chat_models.openai.ChatOpenAI.completion_with_retry.<locals>._completion_with_retry in 4.0 seconds as it raised RateLimitError: That model is currently overloaded with other requests. You can retry your request, or contact us through our help center at help.openai.com if the error persists. (Please include the request ID a0c60585d03ab1f548ad6fca2e1055c2 in your message.).\n",
      "Retrying langchain.chat_models.openai.ChatOpenAI.completion_with_retry.<locals>._completion_with_retry in 4.0 seconds as it raised RateLimitError: That model is currently overloaded with other requests. You can retry your request, or contact us through our help center at help.openai.com if the error persists. (Please include the request ID 31e25d1babdf36b404cbd8ef66e2499e in your message.).\n",
      "Retrying langchain.chat_models.openai.ChatOpenAI.completion_with_retry.<locals>._completion_with_retry in 4.0 seconds as it raised RateLimitError: That model is currently overloaded with other requests. You can retry your request, or contact us through our help center at help.openai.com if the error persists. (Please include the request ID 8cbf43de865d0e6e7d27bcf9f638b097 in your message.).\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6ccfa27a62034655ba8a3362a9441b61",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Creating json from Arrow format:   0%|          | 0/1 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "79e68666b016451ba8f5b9521a4c1e1e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/100 [00:00<?, ?ex/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "611acf5f6ccd4b05a83d16f1b74c9ea9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Creating json from Arrow format:   0%|          | 0/1 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating worldtree...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7a051b1615674dfd9fe7802f9cd59286",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/100 [00:00<?, ?ex/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0f2411163a5a479cb121e6bde8dc6bab",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Creating json from Arrow format:   0%|          | 0/1 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0bfc91852df64c3a991542e82c85cd4d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/100 [00:00<?, ?ex/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cd22b4f5cebd47c0a4fdf1b77f3f779c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Creating json from Arrow format:   0%|          | 0/1 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "config={\n",
    "    \"instruction_keys\": None,\n",
    "    \"cot_trigger_keys\": [\"zhou-01\"],\n",
    "    \"answer_extraction_keys\": 'auto-kojima', \n",
    "    \"author\" : \"thoughtsource\",\n",
    "    \"api_service\": \"openai_chat\",\n",
    "    \"api_time_interval\": 1,\n",
    "    \"engine\": \"gpt-3.5-turbo\", \n",
    "    \"temperature\": 0,\n",
    "    \"max_tokens\": 512,\n",
    "    \"verbose\": False,\n",
    "    \"warn\": False,\n",
    "}\n",
    "medmc.generate(config=config)\n",
    "medmc.dump(\"medmc_zhou_chatgpt.json\")\n",
    "medmc.evaluate()\n",
    "medmc.dump(\"medmc_zhou_eval_chatgpt.json\")\n",
    "\n",
    "openbook.generate(config=config)\n",
    "openbook.dump(\"openbook_zhou_chatgpt.json\")\n",
    "openbook.evaluate()\n",
    "openbook.dump(\"openbook_zhou_eval_chatgpt.json\")\n",
    "\n",
    "worldtree.generate(config=config)\n",
    "worldtree.dump(\"worldtree_zhou_chatgpt.json\")\n",
    "worldtree.evaluate()\n",
    "worldtree.dump(\"worldtree_zhou_eval_chatgpt.json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "eadcd9f7dbdc4448ab80b177b61a50f9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/100 [00:00<?, ?ex/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "{'medmc_qa': {'validation': {'accuracy': {'gpt-3.5-turbo': {'None_zhou-01_kojima-A-D': 0.53}}}}}"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "medmc.evaluate()\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8b6a85bf0507437287a62a8c7509ebd2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/100 [00:00<?, ?ex/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "{'worldtree': {'test': {'accuracy': {'gpt-3.5-turbo': {'None_zhou-01_kojima-A-D': 0.94}}}}}"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "worldtree.evaluate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7950b58f1aef45ef9f5bbdd374f60bca",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/100 [00:00<?, ?ex/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "{'open_book_qa': {'test': {'accuracy': {'gpt-3.5-turbo': {'None_zhou-01_kojima-A-D': 0.72}}}}}"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "openbook.evaluate()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from cot import Collection\n",
    "medqa = Collection.from_json(\"med_qa_zhou.json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# medqa = medqa.select(split=\"test\", number_samples=1, random_samples=True, seed=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating med_qa...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "304d096419854e368f1ee16ab60c4610",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/100 [00:00<?, ?ex/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Retrying langchain.chat_models.openai.ChatOpenAI.completion_with_retry.<locals>._completion_with_retry in 4.0 seconds as it raised RateLimitError: That model is currently overloaded with other requests. You can retry your request, or contact us through our help center at help.openai.com if the error persists. (Please include the request ID e7946cc1dffd625dd570bc6de89f1748 in your message.).\n",
      "Retrying langchain.chat_models.openai.ChatOpenAI.completion_with_retry.<locals>._completion_with_retry in 4.0 seconds as it raised RateLimitError: That model is currently overloaded with other requests. You can retry your request, or contact us through our help center at help.openai.com if the error persists. (Please include the request ID b41d0f2f4fa244d4dc620d9c5a33ad3e in your message.).\n",
      "Retrying langchain.chat_models.openai.ChatOpenAI.completion_with_retry.<locals>._completion_with_retry in 4.0 seconds as it raised RateLimitError: That model is currently overloaded with other requests. You can retry your request, or contact us through our help center at help.openai.com if the error persists. (Please include the request ID 70ab28fd0431a57142da3cc4e65afa30 in your message.).\n",
      "Retrying langchain.chat_models.openai.ChatOpenAI.completion_with_retry.<locals>._completion_with_retry in 4.0 seconds as it raised RateLimitError: That model is currently overloaded with other requests. You can retry your request, or contact us through our help center at help.openai.com if the error persists. (Please include the request ID 6a60eefc4f0ff0a18ea49183c0e21698 in your message.).\n"
     ]
    }
   ],
   "source": [
    "config={\n",
    "    \"instruction_keys\": None,\n",
    "    \"cot_trigger_keys\": [\"zhou-refl\"],\n",
    "    \"answer_extraction_keys\": 'auto-kojima', \n",
    "    \"author\" : \"thoughtsource\",\n",
    "    \"api_service\": \"openai_chat\",\n",
    "    \"api_time_interval\": 1,\n",
    "    \"engine\": \"gpt-3.5-turbo\", \n",
    "    \"temperature\": 0,\n",
    "    \"max_tokens\": 512,\n",
    "    \"verbose\": False,\n",
    "    \"warn\": False,\n",
    "}\n",
    "medqa.generate(config=config)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5ae4af514bf844deb9b9a9bed7e01fc6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Creating json from Arrow format:   0%|          | 0/1 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "medqa.dump(\"med_qa_zhou_refl_chatgpt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5bccb16110b843a8a55b6539d978677e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/100 [00:00<?, ?ex/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "{'med_qa': {'test': {'accuracy': {'gpt-3.5-turbo': {'None_zhou-01_kojima-A-E': 0.58,\n",
       "     'None_zhou-refl_kojima-A-E': 0.52}}}}}"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "medqa.evaluate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating med_qa...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d7345ed3485948198427b737257ef939",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/100 [00:00<?, ?ex/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "config={\n",
    "    \"instruction_keys\": None,\n",
    "    \"cot_trigger_keys\": [\"zhou-refl\"],\n",
    "    \"answer_extraction_keys\": 'auto-kojima', \n",
    "    \"author\" : \"thoughtsource\",\n",
    "    \"api_service\": \"openai_chat\",\n",
    "    \"api_time_interval\": 1,\n",
    "    \"engine\": \"gpt-4\", \n",
    "    \"temperature\": 0,\n",
    "    \"max_tokens\": 512,\n",
    "    \"verbose\": False,\n",
    "    \"warn\": False,\n",
    "}\n",
    "medqa.generate(config=config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "52e97b7a424a43bb8922d521ff6ed208",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/100 [00:00<?, ?ex/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5f32d9eb3abc42578250fbbd33db4bef",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Creating json from Arrow format:   0%|          | 0/1 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5b53b7d9f26246fbb61af1286e75372c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/100 [00:00<?, ?ex/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "{'med_qa': {'test': {'accuracy': {'gpt-3.5-turbo': {'None_zhou-01_kojima-A-E': 0.58,\n",
       "     'None_zhou-refl_kojima-A-E': 0.52},\n",
       "    'gpt-4': {'None_zhou-refl_kojima-A-E': 0.8}}}}}"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "medqa.evaluate()\n",
    "medqa.dump(\"gpt_4_zhou_refl\")\n",
    "medqa.evaluate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "260b4c8fc1e84637975c51029005a49e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/100 [00:00<?, ?ex/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "{'med_qa': {'test': {'accuracy': {'gpt-4': {'None_zhou-01_kojima-A-E': 0.81}}}}}"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test = Collection.from_json(\"med_qa_zhou_gpt4.json\")\n",
    "test.evaluate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "med_qa_false = Collection.from_json(\"/Users/robertpraas/Desktop/ThoughtSource/notebooks/reflection_messages/med_qa_zhou_eval_gpt4.json\")\n",
    "med_qa_false.select_generated_cots(answer=False)\n",
    "med_qa_false = med_qa_false.filter(lambda x: len(x[\"generated_cot\"])==1)\n",
    "med_qa_false.dump(\"medqa_false_gpt_4\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run TS-400"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "| Name   | Train   | Valid   |   Test |\n",
       "|--------|---------|---------|--------|\n",
       "| med_qa | -       | -       |    400 |\n",
       "\n",
       "Not loaded: ['aqua', 'asdiv', 'commonsense_qa', 'entailment_bank', 'gsm8k', 'mawps', 'med_qa_open', 'medmc_qa', '_init_', 'mmlu_clinical_knowledge', 'mmlu_college_biology', 'mmlu_college_medicine', 'mmlu_medical_genetics', 'mmlu_professional_medicine', '_init_', 'mmlu_anatomy', 'open_book_qa', 'pubmed_qa', 'qed', 'strategy_qa', 'svamp', 'worldtree']"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from cot import Collection\n",
    "med_qa = Collection.from_json(\"med_qa_400.json\")\n",
    "\n",
    "med_qa"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "config={\n",
    "    \"instruction_keys\": None,\n",
    "    \"cot_trigger_keys\": [\"zhou-01\"],\n",
    "    \"answer_extraction_keys\": 'auto-kojima', \n",
    "    \"author\" : \"thoughtsource\",\n",
    "    \"api_service\": \"openai_chat\",\n",
    "    \"api_time_interval\": 1,\n",
    "    \"engine\": \"gpt-3.5-turbo\", \n",
    "    \"temperature\": 0,\n",
    "    \"max_tokens\": 512,\n",
    "    \"verbose\": False,\n",
    "    \"warn\": False,\n",
    "}\n",
    "med_qa.generate(config=config)\n",
    "med_qa.dump(\"med_qa_400_turbo\")\n",
    "med_qa.evaluate()\n",
    "med_qa.dump(\"med_qa_400_turbo_eval\")\n",
    "med_qa.evaluate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'id': 'med_qa_test_3',\n",
       " 'ref_id': '',\n",
       " 'question': 'A 39-year-old woman is brought to the emergency department because of fevers, chills, and left lower quadrant pain. Her temperature is 39.1C (102.3F), pulse is 126/min, respirations are 28/min, and blood pressure is 80/50 mm Hg. There is blood oozing around the site of a peripheral intravenous line. Pelvic examination shows mucopurulent discharge from the cervical os and left adnexal tenderness. Laboratory studies show:\\nPlatelet count 14,200/mm3\\nFibrinogen 83 mg/mL (N = 200430 mg/dL)\\nD-dimer 965 ng/mL (N < 500 ng/mL)\\nWhen phenol is applied to a sample of the patient\\'s blood at 90C, a phosphorylated N-acetylglucosamine dimer with 6 fatty acids attached to a polysaccharide side chain is identified. A blood culture is most likely to show which of the following?\"',\n",
       " 'type': 'multiplechoice',\n",
       " 'choices': ['Coagulase-positive, gram-positive cocci forming mauve-colored colonies on methicillin-containing agar',\n",
       "  'Encapsulated, gram-negative coccobacilli forming grey-colored colonies on charcoal blood agar',\n",
       "  'Spore-forming, gram-positive bacilli forming yellow colonies on casein agar',\n",
       "  'Lactose-fermenting, gram-negative rods forming pink colonies on MacConkey agar',\n",
       "  'Gamma-hemolytic, gram-positive cocci forming green colonies on vancomycin agar'],\n",
       " 'context': '',\n",
       " 'cot': [],\n",
       " 'answer': ['Lactose-fermenting, gram-negative rods forming pink colonies on MacConkey agar'],\n",
       " 'generated_cot': [],\n",
       " 'feedback': []}"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "med_qa['med_qa']['test'][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'id': 'med_qa_test_3',\n",
       " 'ref_id': '',\n",
       " 'question': 'A 39-year-old woman is brought to the emergency department because of fevers, chills, and left lower quadrant pain. Her temperature is 39.1C (102.3F), pulse is 126/min, respirations are 28/min, and blood pressure is 80/50 mm Hg. There is blood oozing around the site of a peripheral intravenous line. Pelvic examination shows mucopurulent discharge from the cervical os and left adnexal tenderness. Laboratory studies show:\\nPlatelet count 14,200/mm3\\nFibrinogen 83 mg/mL (N = 200430 mg/dL)\\nD-dimer 965 ng/mL (N < 500 ng/mL)\\nWhen phenol is applied to a sample of the patient\\'s blood at 90C, a phosphorylated N-acetylglucosamine dimer with 6 fatty acids attached to a polysaccharide side chain is identified. A blood culture is most likely to show which of the following?\"',\n",
       " 'type': 'multiplechoice',\n",
       " 'choices': ['Coagulase-positive, gram-positive cocci forming mauve-colored colonies on methicillin-containing agar',\n",
       "  'Encapsulated, gram-negative coccobacilli forming grey-colored colonies on charcoal blood agar',\n",
       "  'Spore-forming, gram-positive bacilli forming yellow colonies on casein agar',\n",
       "  'Lactose-fermenting, gram-negative rods forming pink colonies on MacConkey agar',\n",
       "  'Gamma-hemolytic, gram-positive cocci forming green colonies on vancomycin agar'],\n",
       " 'context': '',\n",
       " 'cot': [],\n",
       " 'answer': ['Lactose-fermenting, gram-negative rods forming pink colonies on MacConkey agar'],\n",
       " 'generated_cot': [],\n",
       " 'feedback': []}"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "med_qa['med_qa']['test'][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating med_qa...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "759ec8fccd8c46839e9d4153771543e5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/400 [00:00<?, ?ex/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(API-)Error in item 0: blocked output: please adjust your prompt and try again, as this generation may be a potential violation of our Usage Guidelines (https://docs.cohere.ai/usage-guidelines/).\n",
      "Retrying with additional time of 10 seconds.\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[8], line 20\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[39m# generate cohere\u001b[39;00m\n\u001b[1;32m      7\u001b[0m config\u001b[39m=\u001b[39m{\n\u001b[1;32m      8\u001b[0m     \u001b[39m\"\u001b[39m\u001b[39minstruction_keys\u001b[39m\u001b[39m\"\u001b[39m: \u001b[39mNone\u001b[39;00m,\n\u001b[1;32m      9\u001b[0m     \u001b[39m\"\u001b[39m\u001b[39mcot_trigger_keys\u001b[39m\u001b[39m\"\u001b[39m: [\u001b[39m\"\u001b[39m\u001b[39mzhou-01\u001b[39m\u001b[39m\"\u001b[39m],\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     18\u001b[0m     \u001b[39m\"\u001b[39m\u001b[39mwarn\u001b[39m\u001b[39m\"\u001b[39m: \u001b[39mFalse\u001b[39;00m,\n\u001b[1;32m     19\u001b[0m }\n\u001b[0;32m---> 20\u001b[0m med_qa\u001b[39m.\u001b[39;49mgenerate(config\u001b[39m=\u001b[39;49mconfig)\n\u001b[1;32m     22\u001b[0m med_qa\u001b[39m.\u001b[39mdump(\u001b[39m\"\u001b[39m\u001b[39mmed_qa_400_zhou_cohere.json\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m     23\u001b[0m med_qa\u001b[39m.\u001b[39mevaluate()\n",
      "File \u001b[0;32m~/ThoughtSource/libs/cot/cot/dataloader.py:548\u001b[0m, in \u001b[0;36mCollection.generate\u001b[0;34m(self, name, split, config)\u001b[0m\n\u001b[1;32m    546\u001b[0m         \u001b[39mprint\u001b[39m(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mGenerating \u001b[39m\u001b[39m{\u001b[39;00mname\u001b[39m}\u001b[39;00m\u001b[39m...\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m    547\u001b[0m         \u001b[39mfor\u001b[39;00m split \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_cache[name]:\n\u001b[0;32m--> 548\u001b[0m             \u001b[39mself\u001b[39m[name][split] \u001b[39m=\u001b[39m generate_and_extract(\u001b[39mself\u001b[39;49m[name][split], config\u001b[39m=\u001b[39;49mconfig)\n\u001b[1;32m    549\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m    550\u001b[0m     \u001b[39mif\u001b[39;00m split \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n",
      "File \u001b[0;32m~/ThoughtSource/libs/cot/cot/generate.py:63\u001b[0m, in \u001b[0;36mgenerate_and_extract\u001b[0;34m(data, config)\u001b[0m\n\u001b[1;32m     59\u001b[0m \u001b[39m# The config is transformed into a dataclass object, where all testing is done\u001b[39;00m\n\u001b[1;32m     60\u001b[0m \u001b[39m# But it will be transformed back to a dictionary for the function 'map'\u001b[39;00m\n\u001b[1;32m     61\u001b[0m config_as_dataclass \u001b[39m=\u001b[39m Config(\u001b[39m*\u001b[39m\u001b[39m*\u001b[39madaptive_config)\n\u001b[0;32m---> 63\u001b[0m \u001b[39mreturn\u001b[39;00m data\u001b[39m.\u001b[39;49mmap(\n\u001b[1;32m     64\u001b[0m     _generate_and_extract,\n\u001b[1;32m     65\u001b[0m     with_indices\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m,\n\u001b[1;32m     66\u001b[0m     fn_kwargs\u001b[39m=\u001b[39;49masdict(config_as_dataclass),\n\u001b[1;32m     67\u001b[0m     features\u001b[39m=\u001b[39;49mfeatures,\n\u001b[1;32m     68\u001b[0m     load_from_cache_file\u001b[39m=\u001b[39;49m\u001b[39mFalse\u001b[39;49;00m,\n\u001b[1;32m     69\u001b[0m )\n",
      "File \u001b[0;32m~/ThoughtSource/.conda/lib/python3.9/site-packages/datasets/arrow_dataset.py:1955\u001b[0m, in \u001b[0;36mDataset.map\u001b[0;34m(self, function, with_indices, with_rank, input_columns, batched, batch_size, drop_last_batch, remove_columns, keep_in_memory, load_from_cache_file, cache_file_name, writer_batch_size, features, disable_nullable, fn_kwargs, num_proc, suffix_template, new_fingerprint, desc)\u001b[0m\n\u001b[1;32m   1952\u001b[0m disable_tqdm \u001b[39m=\u001b[39m \u001b[39mnot\u001b[39;00m logging\u001b[39m.\u001b[39mis_progress_bar_enabled()\n\u001b[1;32m   1954\u001b[0m \u001b[39mif\u001b[39;00m num_proc \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39mor\u001b[39;00m num_proc \u001b[39m==\u001b[39m \u001b[39m1\u001b[39m:\n\u001b[0;32m-> 1955\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_map_single(\n\u001b[1;32m   1956\u001b[0m         function\u001b[39m=\u001b[39;49mfunction,\n\u001b[1;32m   1957\u001b[0m         with_indices\u001b[39m=\u001b[39;49mwith_indices,\n\u001b[1;32m   1958\u001b[0m         with_rank\u001b[39m=\u001b[39;49mwith_rank,\n\u001b[1;32m   1959\u001b[0m         input_columns\u001b[39m=\u001b[39;49minput_columns,\n\u001b[1;32m   1960\u001b[0m         batched\u001b[39m=\u001b[39;49mbatched,\n\u001b[1;32m   1961\u001b[0m         batch_size\u001b[39m=\u001b[39;49mbatch_size,\n\u001b[1;32m   1962\u001b[0m         drop_last_batch\u001b[39m=\u001b[39;49mdrop_last_batch,\n\u001b[1;32m   1963\u001b[0m         remove_columns\u001b[39m=\u001b[39;49mremove_columns,\n\u001b[1;32m   1964\u001b[0m         keep_in_memory\u001b[39m=\u001b[39;49mkeep_in_memory,\n\u001b[1;32m   1965\u001b[0m         load_from_cache_file\u001b[39m=\u001b[39;49mload_from_cache_file,\n\u001b[1;32m   1966\u001b[0m         cache_file_name\u001b[39m=\u001b[39;49mcache_file_name,\n\u001b[1;32m   1967\u001b[0m         writer_batch_size\u001b[39m=\u001b[39;49mwriter_batch_size,\n\u001b[1;32m   1968\u001b[0m         features\u001b[39m=\u001b[39;49mfeatures,\n\u001b[1;32m   1969\u001b[0m         disable_nullable\u001b[39m=\u001b[39;49mdisable_nullable,\n\u001b[1;32m   1970\u001b[0m         fn_kwargs\u001b[39m=\u001b[39;49mfn_kwargs,\n\u001b[1;32m   1971\u001b[0m         new_fingerprint\u001b[39m=\u001b[39;49mnew_fingerprint,\n\u001b[1;32m   1972\u001b[0m         disable_tqdm\u001b[39m=\u001b[39;49mdisable_tqdm,\n\u001b[1;32m   1973\u001b[0m         desc\u001b[39m=\u001b[39;49mdesc,\n\u001b[1;32m   1974\u001b[0m     )\n\u001b[1;32m   1975\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m   1977\u001b[0m     \u001b[39mdef\u001b[39;00m \u001b[39mformat_cache_file_name\u001b[39m(cache_file_name, rank):\n",
      "File \u001b[0;32m~/ThoughtSource/.conda/lib/python3.9/site-packages/datasets/arrow_dataset.py:520\u001b[0m, in \u001b[0;36mtransmit_tasks.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    518\u001b[0m     \u001b[39mself\u001b[39m: \u001b[39m\"\u001b[39m\u001b[39mDataset\u001b[39m\u001b[39m\"\u001b[39m \u001b[39m=\u001b[39m kwargs\u001b[39m.\u001b[39mpop(\u001b[39m\"\u001b[39m\u001b[39mself\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m    519\u001b[0m \u001b[39m# apply actual function\u001b[39;00m\n\u001b[0;32m--> 520\u001b[0m out: Union[\u001b[39m\"\u001b[39m\u001b[39mDataset\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39m\"\u001b[39m\u001b[39mDatasetDict\u001b[39m\u001b[39m\"\u001b[39m] \u001b[39m=\u001b[39m func(\u001b[39mself\u001b[39;49m, \u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m    521\u001b[0m datasets: List[\u001b[39m\"\u001b[39m\u001b[39mDataset\u001b[39m\u001b[39m\"\u001b[39m] \u001b[39m=\u001b[39m \u001b[39mlist\u001b[39m(out\u001b[39m.\u001b[39mvalues()) \u001b[39mif\u001b[39;00m \u001b[39misinstance\u001b[39m(out, \u001b[39mdict\u001b[39m) \u001b[39melse\u001b[39;00m [out]\n\u001b[1;32m    522\u001b[0m \u001b[39mfor\u001b[39;00m dataset \u001b[39min\u001b[39;00m datasets:\n\u001b[1;32m    523\u001b[0m     \u001b[39m# Remove task templates if a column mapping of the template is no longer valid\u001b[39;00m\n",
      "File \u001b[0;32m~/ThoughtSource/.conda/lib/python3.9/site-packages/datasets/arrow_dataset.py:487\u001b[0m, in \u001b[0;36mtransmit_format.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    480\u001b[0m self_format \u001b[39m=\u001b[39m {\n\u001b[1;32m    481\u001b[0m     \u001b[39m\"\u001b[39m\u001b[39mtype\u001b[39m\u001b[39m\"\u001b[39m: \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_format_type,\n\u001b[1;32m    482\u001b[0m     \u001b[39m\"\u001b[39m\u001b[39mformat_kwargs\u001b[39m\u001b[39m\"\u001b[39m: \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_format_kwargs,\n\u001b[1;32m    483\u001b[0m     \u001b[39m\"\u001b[39m\u001b[39mcolumns\u001b[39m\u001b[39m\"\u001b[39m: \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_format_columns,\n\u001b[1;32m    484\u001b[0m     \u001b[39m\"\u001b[39m\u001b[39moutput_all_columns\u001b[39m\u001b[39m\"\u001b[39m: \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_output_all_columns,\n\u001b[1;32m    485\u001b[0m }\n\u001b[1;32m    486\u001b[0m \u001b[39m# apply actual function\u001b[39;00m\n\u001b[0;32m--> 487\u001b[0m out: Union[\u001b[39m\"\u001b[39m\u001b[39mDataset\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39m\"\u001b[39m\u001b[39mDatasetDict\u001b[39m\u001b[39m\"\u001b[39m] \u001b[39m=\u001b[39m func(\u001b[39mself\u001b[39;49m, \u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m    488\u001b[0m datasets: List[\u001b[39m\"\u001b[39m\u001b[39mDataset\u001b[39m\u001b[39m\"\u001b[39m] \u001b[39m=\u001b[39m \u001b[39mlist\u001b[39m(out\u001b[39m.\u001b[39mvalues()) \u001b[39mif\u001b[39;00m \u001b[39misinstance\u001b[39m(out, \u001b[39mdict\u001b[39m) \u001b[39melse\u001b[39;00m [out]\n\u001b[1;32m    489\u001b[0m \u001b[39m# re-apply format to the output\u001b[39;00m\n",
      "File \u001b[0;32m~/ThoughtSource/.conda/lib/python3.9/site-packages/datasets/fingerprint.py:458\u001b[0m, in \u001b[0;36mfingerprint_transform.<locals>._fingerprint.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    452\u001b[0m             kwargs[fingerprint_name] \u001b[39m=\u001b[39m update_fingerprint(\n\u001b[1;32m    453\u001b[0m                 \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_fingerprint, transform, kwargs_for_fingerprint\n\u001b[1;32m    454\u001b[0m             )\n\u001b[1;32m    456\u001b[0m \u001b[39m# Call actual function\u001b[39;00m\n\u001b[0;32m--> 458\u001b[0m out \u001b[39m=\u001b[39m func(\u001b[39mself\u001b[39;49m, \u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m    460\u001b[0m \u001b[39m# Update fingerprint of in-place transforms + update in-place history of transforms\u001b[39;00m\n\u001b[1;32m    462\u001b[0m \u001b[39mif\u001b[39;00m inplace:  \u001b[39m# update after calling func so that the fingerprint doesn't change if the function fails\u001b[39;00m\n",
      "File \u001b[0;32m~/ThoughtSource/.conda/lib/python3.9/site-packages/datasets/arrow_dataset.py:2320\u001b[0m, in \u001b[0;36mDataset._map_single\u001b[0;34m(self, function, with_indices, with_rank, input_columns, batched, batch_size, drop_last_batch, remove_columns, keep_in_memory, load_from_cache_file, cache_file_name, writer_batch_size, features, disable_nullable, fn_kwargs, new_fingerprint, rank, offset, disable_tqdm, desc, cache_only)\u001b[0m\n\u001b[1;32m   2318\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m batched:\n\u001b[1;32m   2319\u001b[0m     \u001b[39mfor\u001b[39;00m i, example \u001b[39min\u001b[39;00m \u001b[39menumerate\u001b[39m(pbar):\n\u001b[0;32m-> 2320\u001b[0m         example \u001b[39m=\u001b[39m apply_function_on_filtered_inputs(example, i, offset\u001b[39m=\u001b[39;49moffset)\n\u001b[1;32m   2321\u001b[0m         \u001b[39mif\u001b[39;00m update_data:\n\u001b[1;32m   2322\u001b[0m             \u001b[39mif\u001b[39;00m i \u001b[39m==\u001b[39m \u001b[39m0\u001b[39m:\n",
      "File \u001b[0;32m~/ThoughtSource/.conda/lib/python3.9/site-packages/datasets/arrow_dataset.py:2220\u001b[0m, in \u001b[0;36mDataset._map_single.<locals>.apply_function_on_filtered_inputs\u001b[0;34m(inputs, indices, check_same_num_examples, offset)\u001b[0m\n\u001b[1;32m   2218\u001b[0m \u001b[39mif\u001b[39;00m with_rank:\n\u001b[1;32m   2219\u001b[0m     additional_args \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m (rank,)\n\u001b[0;32m-> 2220\u001b[0m processed_inputs \u001b[39m=\u001b[39m function(\u001b[39m*\u001b[39;49mfn_args, \u001b[39m*\u001b[39;49madditional_args, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mfn_kwargs)\n\u001b[1;32m   2221\u001b[0m \u001b[39mif\u001b[39;00m update_data \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m   2222\u001b[0m     \u001b[39m# Check if the function returns updated examples\u001b[39;00m\n\u001b[1;32m   2223\u001b[0m     update_data \u001b[39m=\u001b[39m \u001b[39misinstance\u001b[39m(processed_inputs, (Mapping, pa\u001b[39m.\u001b[39mTable))\n",
      "File \u001b[0;32m~/ThoughtSource/.conda/lib/python3.9/site-packages/datasets/arrow_dataset.py:1915\u001b[0m, in \u001b[0;36mDataset.map.<locals>.decorate.<locals>.decorated\u001b[0;34m(item, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1911\u001b[0m decorated_item \u001b[39m=\u001b[39m (\n\u001b[1;32m   1912\u001b[0m     Example(item, features\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mfeatures) \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m batched \u001b[39melse\u001b[39;00m Batch(item, features\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mfeatures)\n\u001b[1;32m   1913\u001b[0m )\n\u001b[1;32m   1914\u001b[0m \u001b[39m# Use the LazyDict internally, while mapping the function\u001b[39;00m\n\u001b[0;32m-> 1915\u001b[0m result \u001b[39m=\u001b[39m f(decorated_item, \u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1916\u001b[0m \u001b[39m# Return a standard dict\u001b[39;00m\n\u001b[1;32m   1917\u001b[0m \u001b[39mreturn\u001b[39;00m result\u001b[39m.\u001b[39mdata \u001b[39mif\u001b[39;00m \u001b[39misinstance\u001b[39m(result, LazyDict) \u001b[39melse\u001b[39;00m result\n",
      "File \u001b[0;32m~/ThoughtSource/libs/cot/cot/generate.py:162\u001b[0m, in \u001b[0;36m_generate_and_extract\u001b[0;34m(item, idx, idx_range, author, api_service, engine, temperature, max_tokens, api_time_interval, instruction_keys, cot_trigger_keys, template_cot_generation, answer_extraction_keys, template_answer_extraction, warn, verbose)\u001b[0m\n\u001b[1;32m    159\u001b[0m     \u001b[39mprint\u001b[39m(\u001b[39m\"\u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39m-----------------COT TRIGGER TEXT-----------------\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m    160\u001b[0m     \u001b[39mprint\u001b[39m(generate_cot_prompt)\n\u001b[0;32m--> 162\u001b[0m cot \u001b[39m=\u001b[39m query_model(\n\u001b[1;32m    163\u001b[0m     generate_cot_prompt,\n\u001b[1;32m    164\u001b[0m     api_service,\n\u001b[1;32m    165\u001b[0m     engine,\n\u001b[1;32m    166\u001b[0m     temperature,\n\u001b[1;32m    167\u001b[0m     max_tokens,\n\u001b[1;32m    168\u001b[0m     api_time_interval,\n\u001b[1;32m    169\u001b[0m )\n\u001b[1;32m    170\u001b[0m \u001b[39mif\u001b[39;00m verbose:\n\u001b[1;32m    171\u001b[0m     \u001b[39mprint\u001b[39m(\u001b[39m\"\u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39m------------------GENERATED COT-------------------\u001b[39m\u001b[39m\"\u001b[39m)\n",
      "File \u001b[0;32m~/ThoughtSource/libs/cot/cot/generate.py:501\u001b[0m, in \u001b[0;36mquery_model\u001b[0;34m(input, api_service, engine, temperature, max_tokens, api_time_interval)\u001b[0m\n\u001b[1;32m    495\u001b[0m     \u001b[39m# return (\"This is a \" + 20 * \"long \" + \"Mock CoT.\\n\")*20\u001b[39;00m\n\u001b[1;32m    496\u001b[0m \n\u001b[1;32m    497\u001b[0m \u001b[39m# langchain package implementation\u001b[39;00m\n\u001b[1;32m    498\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m    499\u001b[0m     \u001b[39mfrom\u001b[39;00m \u001b[39mlangchain\u001b[39;00m \u001b[39mimport\u001b[39;00m LLMChain, Prompt\n\u001b[0;32m--> 501\u001b[0m     time\u001b[39m.\u001b[39;49msleep(api_time_interval)\n\u001b[1;32m    502\u001b[0m     template \u001b[39m=\u001b[39m \u001b[39m\"\u001b[39m\u001b[39m{prompt}\u001b[39;00m\u001b[39m\"\u001b[39m\n\u001b[1;32m    503\u001b[0m     prompt \u001b[39m=\u001b[39m Prompt(template\u001b[39m=\u001b[39mtemplate, input_variables\u001b[39m=\u001b[39m[\u001b[39m\"\u001b[39m\u001b[39mprompt\u001b[39m\u001b[39m\"\u001b[39m])\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "from cot import Collection\n",
    "med_qa = Collection.from_json(\"med_qa_400.json\")\n",
    "#med_qa = med_qa.select(split=\"test\", number_samples=5, random_samples=True, seed=0)\n",
    "\n",
    "med_qa\n",
    "# generate cohere\n",
    "config={\n",
    "    \"instruction_keys\": None,\n",
    "    \"cot_trigger_keys\": [\"zhou-01\"],\n",
    "    \"answer_extraction_keys\": 'auto-kojima', \n",
    "    \"author\" : \"thoughtsource\",\n",
    "    \"api_service\": \"cohere\",\n",
    "    \"api_time_interval\": 1,\n",
    "    \"engine\": \"command-xlarge-nightly\", \n",
    "    \"temperature\": 0,\n",
    "    \"max_tokens\": 512,\n",
    "    \"verbose\": False,\n",
    "    \"warn\": False,\n",
    "}\n",
    "med_qa.generate(config=config)\n",
    "\n",
    "med_qa.dump(\"med_qa_400_zhou_cohere.json\")\n",
    "med_qa.evaluate()\n",
    "med_qa.dump(\"med_qa_400_zhou_cohere.json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from cot import Collection\n",
    "med_qa = Collection.from_json(\"med_qa_400.json\")\n",
    "config={\n",
    "    \"instruction_keys\": None,\n",
    "    \"cot_trigger_keys\": [\"zhou-01\"],\n",
    "    \"answer_extraction_keys\": 'auto-kojima', \n",
    "    \"author\" : \"thoughtsource\",\n",
    "    \"api_service\": \"openai_chat\",\n",
    "    \"api_time_interval\": 1,\n",
    "    \"engine\": \"gpt-4\", \n",
    "    \"temperature\": 0,\n",
    "    \"max_tokens\": 512,\n",
    "    \"verbose\": False,\n",
    "    \"warn\": False,\n",
    "}\n",
    "med_qa.generate(config=config)\n",
    "\n",
    "med_qa.dump(\"med_qa_zhou_gpt4.json\")\n",
    "med_qa.evaluate()\n",
    "med_qa.dump(\"med_qa_zhou_eval_gpt4.json\")\n",
    "\n",
    "med_qa.evaluate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check if merging CoTs works"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test template"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = Collection.from_json(\"./med_qa_400_turbo_eval.json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'id': 'med_qa_test_3',\n",
       " 'ref_id': '',\n",
       " 'question': 'A 39-year-old woman is brought to the emergency department because of fevers, chills, and left lower quadrant pain. Her temperature is 39.1C (102.3F), pulse is 126/min, respirations are 28/min, and blood pressure is 80/50 mm Hg. There is blood oozing around the site of a peripheral intravenous line. Pelvic examination shows mucopurulent discharge from the cervical os and left adnexal tenderness. Laboratory studies show:\\nPlatelet count 14,200/mm3\\nFibrinogen 83 mg/mL (N = 200430 mg/dL)\\nD-dimer 965 ng/mL (N < 500 ng/mL)\\nWhen phenol is applied to a sample of the patient\\'s blood at 90C, a phosphorylated N-acetylglucosamine dimer with 6 fatty acids attached to a polysaccharide side chain is identified. A blood culture is most likely to show which of the following?\"',\n",
       " 'type': 'multiplechoice',\n",
       " 'choices': ['Coagulase-positive, gram-positive cocci forming mauve-colored colonies on methicillin-containing agar',\n",
       "  'Encapsulated, gram-negative coccobacilli forming grey-colored colonies on charcoal blood agar',\n",
       "  'Spore-forming, gram-positive bacilli forming yellow colonies on casein agar',\n",
       "  'Lactose-fermenting, gram-negative rods forming pink colonies on MacConkey agar',\n",
       "  'Gamma-hemolytic, gram-positive cocci forming green colonies on vancomycin agar'],\n",
       " 'context': '',\n",
       " 'cot': [],\n",
       " 'answer': ['Lactose-fermenting, gram-negative rods forming pink colonies on MacConkey agar'],\n",
       " 'generated_cot': [{'id': '96bd6bab-1492-4492-a25a-74206cbd4b11',\n",
       "   'fragments_version': '0.01',\n",
       "   'instruction': None,\n",
       "   'cot_trigger': 'zhou-01',\n",
       "   'cot_trigger_template': '{instruction}\\n\\n{question}\\n{answer_choices}\\n\\n{cot_trigger}',\n",
       "   'prompt_text': '',\n",
       "   'cot': \"First, we can use the information given about the patient's symptoms and laboratory results to narrow down the possible organisms causing her infection. The presence of mucopurulent discharge and left adnexal tenderness suggests a pelvic inflammatory disease (PID), which can be caused by a variety of bacteria including Neisseria gonorrhoeae and Chlamydia trachomatis. The low platelet count and elevated D-dimer suggest disseminated intravascular coagulation (DIC), which can be caused by sepsis from a variety of organisms.\\n\\nNext, we can use the information about the phenol test to further narrow down the possibilities. This test is used to identify the presence of lipopolysaccharides (LPS) on the surface of gram-negative bacteria. The specific structure identified in this case (a phosphorylated N-acetylglucosamine dimer with 6 fatty acids attached to a polysaccharide side chain) is characteristic of the LPS found in gram-negative bacteria of the Enterobacteriaceae family, such as Escherichia coli and Klebsiella pneumoniae.\\n\\nBased on this information, we can eliminate options A, C, and E, as they describe gram-positive bacteria. Option B describes encapsulated gram-negative coccobacilli, which could potentially cause sepsis and DIC, but it does not fit with the information from the phenol test. Option D describes lactose-fermenting gram-negative rods, which could include E. coli and Klebsiella pneumoniae, and is the most likely answer given the information provided.\\n\\nTherefore, the correct answer is D) Lactose-fermenting, gram-negative rods forming pink colonies on MacConkey agar.\",\n",
       "   'answers': [{'id': 'abf4aa11-4576-47dc-b7ea-a284f0908006',\n",
       "     'answer_extraction': 'kojima-A-E',\n",
       "     'answer_extraction_template': '{instruction}\\n\\n{question}\\n{answer_choices}\\n\\n{cot_trigger}{cot}\\n{answer_extraction}',\n",
       "     'answer_extraction_text': '',\n",
       "     'answer': 'D) Lactose-fermenting, gram-negative rods forming pink colonies on MacConkey agar.',\n",
       "     'answer_from_choices': 'D',\n",
       "     'correct_answer': True}],\n",
       "   'author': 'thoughtsource',\n",
       "   'date': '2023/06/01 21:54:43',\n",
       "   'api_service': 'openai_chat',\n",
       "   'model': \"{'name': 'gpt-3.5-turbo', 'temperature': 0, 'max_tokens': 512}\",\n",
       "   'comment': '',\n",
       "   'annotations': []}],\n",
       " 'feedback': []}"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data['med_qa']['test'][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
